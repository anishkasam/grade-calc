{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the finalized exported gradebook from canvas should be stored as grades.csv\n",
    "# the graded final scores from gradescope should be stored as final.csv\n",
    "# both files should be in the same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85187adb-60a5-4d05-8a6b-6b944004d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3645350-0f4a-4d6e-88af-8058248e9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.read_csv(\"grades.csv\")\n",
    "grades = grades.fillna(0)\n",
    "grades.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c59e7-7567-4ac3-8a17-7c82d7382c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of assignments, category weight, lowest scores dropped, and maximum score\n",
    "rq, rq_weight, rq_drops, rq_cap = [], 0.05, 3, 1.0\n",
    "lect, lect_weight, lect_drops, lect_cap = [], 0.02, 6, 1.0\n",
    "disc, disc_weight, disc_drops, disc_cap = [], 0.01, 0, 1.0\n",
    "lab, lab_weight, lab_drops, lab_cap = [], 0.08, 1, 1.0\n",
    "hw, hw_weight, hw_drops, hw_cap = [], 0.14, 0, 1.0\n",
    "proj, proj_weight, proj_drops, proj_cap = [], 0.1, 0, 1.0\n",
    "st, st_weight, st_drops, st_cap = [], 0.15, 1, 1.0\n",
    "\n",
    "# exams (grouped together when enforcing cap, hence the \"dummy\" cap)\n",
    "mt, mt_weight, mt_drops, mt_cap = [], 0.15, 0, 2.0\n",
    "final, final_weight, final_drops, final_cap = [], 0.3, 0, 2.0\n",
    "\n",
    "exam_weight, exam_cap = 0.45, 1.0\n",
    "\n",
    "max_pts = grades.iloc[0]\n",
    "pid = [\"SIS User ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca44cf-14b9-4b1d-9665-0c126a7e54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# place columns in respective categories\n",
    "for col in grades.columns:\n",
    "    c = col.lower()\n",
    "    if c.endswith(\"score\"):\n",
    "        continue\n",
    "    elif c.startswith(\"reading quiz\"):\n",
    "        rq.append(col)\n",
    "    elif c.startswith(\"lecture\"):\n",
    "        lect.append(col)\n",
    "    elif c.startswith(\"discussion\"):\n",
    "        disc.append(col)\n",
    "    elif c.startswith(\"lab\"):\n",
    "        lab.append(col)\n",
    "    elif c.startswith(\"hw\"):\n",
    "        hw.append(col)\n",
    "    elif c.startswith(\"project\"):\n",
    "        proj.append(col)\n",
    "    elif c.startswith(\"skill test\"):\n",
    "        st.append(col)\n",
    "    elif c.startswith(\"midterm\"):\n",
    "        mt.append(col)\n",
    "    elif c.startswith(\"final\"):\n",
    "        final.append(col)\n",
    "    elif c.startswith(\"slip day usage\"):\n",
    "        slip_days = col\n",
    "    else:\n",
    "        # print assignments not belonging to any category\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07681560-b1de-41a9-8eda-814e044df3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ungraded assignments from categories\n",
    "lect = [l for l in lect if not l.split()[1] == \"0\"]\n",
    "lab = [l for l in lab if not l.split()[0].endswith(\"00\")]\n",
    "\n",
    "# sanity check to count number of assignments in each category\n",
    "cats = [\"rq\", \"lect\", \"disc\", \"lab\", \"proj\", \"st\", \"final\", \"mt\"]\n",
    "for cat in cats:\n",
    "    print(f\"{cat}: {len(eval(cat))}\", end = \" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7d445-80e2-4712-91bb-f1e05e572643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct final dataframe\n",
    "out = pd.DataFrame()\n",
    "out[\"student\"] = grades[\"Student\"][1:-1]\n",
    "out[\"pid\"] = grades[\"SIS User ID\"][1:-1]\n",
    "out[\"sid\"] = grades[\"SIS Login ID\"][1:-1]\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6b02d-475f-43a2-8a47-302e8c16ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [\"rq\", \"lect\", \"disc\", \"hw\", \"lab\", \"proj\", \"st\", \"final\", \"mt\"]\n",
    "\n",
    "for cat in cats:\n",
    "    # get relevant information for category\n",
    "    assignments = eval(cat)\n",
    "    drops = eval(cat + \"_drops\")\n",
    "    cap = eval(cat + \"_cap\")\n",
    "\n",
    "    # select assignments within category and normalize to proportions\n",
    "    subset = grades[pid + assignments].iloc[0:-1].copy().set_index(pid)\n",
    "    for a in assignments:\n",
    "        subset[a] = subset[a] / subset[a][0]\n",
    "\n",
    "    # calculate category grade (factoring in drops and caps) \n",
    "    total = subset.iloc[0].sum() - drops\n",
    "    compiled = (subset.sum(axis = 1) - subset.apply(lambda x: x.nsmallest(drops).sum(), axis = 1)) / total\n",
    "    compiled = pd.DataFrame(np.clip(compiled, 0, cap))\n",
    "    compiled.columns = [cat + \"_score\"]\n",
    "\n",
    "    # add score back to finalized dataframe\n",
    "    out = out.merge(compiled, left_on = \"pid\", right_index = True, how = \"left\")\n",
    "    \n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e1c5d-b4f0-4d5b-b6f8-302fed84b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate midterm redemption score from the final\n",
    "last_mtq = 14 # this is found from the rubric numbering on gradescope\n",
    "\n",
    "final_df = pd.read_csv(\"final.csv\")\n",
    "makeup_questions = [c for c in final_df.columns if c.split(\":\")[0].split(\".\")[0].isnumeric() and int(c.split(\":\")[0].split(\".\")[0]) <= last_mtq]\n",
    "final_df[\"mt_redemption\"] = final_df[makeup_questions].sum(axis = 1) / max_pts[mt[0]]\n",
    "\n",
    "out = out.merge(final_df[[\"SID\", \"mt_redemption\"]], left_on = \"pid\", right_on = \"SID\", how = \"left\").drop(columns = [\"SID\"])\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ea1ba-60b5-4d74-923e-bf81e72c21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL IF THE FINAL HAS BEEN MOVED TO CANVAS\n",
    "final_df[\"final_score\"] = final_df[\"Total Score\"] / final_df[\"Max Points\"]\n",
    "final_df[[\"SID\", \"final_score\"]]\n",
    "\n",
    "out = out.drop(columns = [\"final_score\"]).merge(final_df[[\"SID\", \"final_score\"]], left_on = \"pid\", right_on = \"SID\", how = \"left\").drop(columns = [\"SID\"])\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67cf8a6-fae6-41bb-b9e8-8e680cd39a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace midterm score with redemption from final if applicable\n",
    "out[\"mt_score\"] = out.apply(lambda x: max(x[\"mt_score\"], x[\"mt_redemption\"]), axis = 1)\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f688d-22a6-4c27-ba77-965395002887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine midterm and final score to calculate exam category\n",
    "out[\"exam_score\"] = out[\"mt_score\"] * (mt_weight / exam_weight) + out[\"final_score\"] * (final_weight / exam_weight)\n",
    "out[\"exam_score\"] = np.clip(out[\"exam_score\"], 0, exam_cap)\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bca1f3-b255-47b8-a6c9-5a6736840821",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ce6b5-3496-4323-be68-e28366c6ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate grade in the course\n",
    "def calc_overall(df):\n",
    "    cats = [\"hw\", \"rq\", \"lect\", \"disc\", \"lab\", \"proj\", \"st\", \"exam\"]\n",
    "    total = [df[cat + \"_score\"] * eval(cat + \"_weight\") for cat in cats]\n",
    "    return sum(total)\n",
    "    \n",
    "out[\"grade\"] = out.apply(calc_overall, axis = 1)\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc5a59-195e-4df0-afb0-63a1425f0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate letter grade in the course\n",
    "def calc_letter_grade(df):\n",
    "    if df[\"final_score\"] < 0.5:\n",
    "        return \"F\"\n",
    "\n",
    "    grade = df[\"grade\"]\n",
    "    if grade >= 0.97:\n",
    "        return \"A+\"\n",
    "    elif grade >= 0.93:\n",
    "        return \"A\"\n",
    "    elif grade >= 0.9:\n",
    "        return \"A-\"\n",
    "    elif grade >= 0.87:\n",
    "        return \"B+\"\n",
    "    elif grade >= 0.83:\n",
    "        return \"B\"\n",
    "    elif grade >= 0.8:\n",
    "        return \"B-\"\n",
    "    elif grade >= 0.77:\n",
    "        return \"C+\"\n",
    "    elif grade >= 0.73:\n",
    "        return \"C\"\n",
    "    elif grade >= 0.7:\n",
    "        return \"C-\"\n",
    "    elif grade >= 0.6:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"F\"\n",
    "\n",
    "out[\"letter_grade\"] = out.apply(calc_letter_grade, axis = 1)\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8ac59-7be7-4181-968a-ce8b1ac34214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gpa from letter grade\n",
    "gpa_mapper = {\"A+\": 4.0, \"A\": 4.0, \"A-\": 3.7, \"B+\": 3.3, \"B\": 3.0, \"B-\": 2.7, \"C+\": 2.3, \"C\": 2.0, \"C-\": 1.7, \"D\": 1.0, \"F\": 0.0}\n",
    "\n",
    "out[\"gpa\"] = out.apply(lambda x: gpa_mapper[x[\"letter_grade\"]], axis = 1)\n",
    "out.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ebff9e-8bc3-4bf6-bdd0-01d02b638596",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"gpa\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04168c50-a788-4530-84f9-a0d402e999d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(\"output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
